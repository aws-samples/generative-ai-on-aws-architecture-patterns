{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee1beb4-8b81-46a1-a4e7-4ddbb62ca2b9",
   "metadata": {},
   "source": [
    "# Access und host Large Language Models on AWS\n",
    "This notebook demostrates how to use LLMs on AWS. \n",
    "\n",
    "There are following options to host and use LLMs/FMs on AWS:\n",
    "1. [Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-service.html)\n",
    "2. [Amazon SageMaker JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models.html)\n",
    "3. Amazon SageMaker-managed hosting using [SageMaker endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html)\n",
    "4. [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/)\n",
    "5. Self-hosted on EC2 containers using [Amazon Elastic Container Service (ECS)](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html) or [Elastic Kubernetes Service (EKS)](https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html)\n",
    "\n",
    "Refer to [Generative AI on AWS](https://aws.amazon.com/generative-ai/) landing page to understand details and use cases for each deployment options. \n",
    "\n",
    "This notebook demonstrates how you can use Amazon Bedrock, Amazon SageMaker JumpStart, and Amazon SageMaker real-time and asynchronous endpoints to host generative AI models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb1b2b5-a7d6-425d-aff4-83d353246435",
   "metadata": {},
   "source": [
    "##Â How to use this notebook\n",
    "You're going to use this notebook for deployment of LLM endpoints through out the workshop for different labs.\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h2><b>This notebook creates cost-incurring resources</b></h2>\n",
    "    <br>\n",
    "    <p style=\"text-align: center; margin: auto;\"><b>You don't need to run this notebook if you're going to use Amazon Bedrock only for LLM access<b></p>\n",
    "        <p></p>\n",
    "    <p style=\"text-align: center; margin: auto;\">Create <b>at least one<b> SageMaker endpoint to host an LLM if you'd like to experiment with endpoints and use the endpoint in a lab</p>\n",
    "    <p style=\"text-align: center; margin: auto;\">You can create more than one inference endpoint. Feel free to experiment but be aware of potential costs</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a04605-2440-48f7-be83-602f9765e27b",
   "metadata": {},
   "source": [
    "## Setup environment\n",
    "Select the _PyTorch 2.0.0 Python 3.10 CPU Optimized_ image for this notebook and `ml.t3.medium` compute instance:\n",
    "\n",
    "![](../static/img/notebook-image-kernel.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baada9f9-2ee6-4c19-831b-03571c9feea9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.132 requires botocore==1.29.132, but you have botocore 1.31.79 which is incompatible.\n",
      "awscli 1.27.132 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0.1 which is incompatible.\n",
      "awscli 1.27.132 requires s3transfer<0.7.0,>=0.6.0, but you have s3transfer 0.7.0 which is incompatible.\n",
      "confection 0.0.4 requires pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4, but you have pydantic 2.4.2 which is incompatible.\n",
      "spacy 3.5.2 requires pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4, but you have pydantic 2.4.2 which is incompatible.\n",
      "spacy 3.5.2 requires typer<0.8.0,>=0.3.0, but you have typer 0.9.0 which is incompatible.\n",
      "thinc 8.1.10 requires pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4, but you have pydantic 2.4.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker boto3 gradio langchain --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245066d5-a2f1-4738-9dd0-c392ae0c17c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart kernel to get the packages\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fec23f74-1c22-44f6-95ae-3593af7ac3a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "2.197.0\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "assert(sagemaker.__version__ >= '2.195.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee5c5054-ea5d-41d8-8295-a0f348859c3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Get some variables you need to interact with SageMaker service\n",
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "bucket_prefix = \"genai-on-aws-workshop\"  \n",
    "sm_session = sagemaker.Session()\n",
    "sm_client = boto_session.client(\"sagemaker\")\n",
    "sm_role = sagemaker.get_execution_role()\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84c2b966-a71c-47d1-b6b0-945380a180c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The notebook tags all deployed resources\n",
    "workshop_tags = [{'Key': 'project', 'Value': 'genai-on-aws-workshop'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1463c684-295e-44c2-a0ba-ba597b74762a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker domain id: d-gg3vgzr0ftzx\n",
      "User profile name: canvas-demo-user\n"
     ]
    }
   ],
   "source": [
    "# Get domain id and user profile\n",
    "NOTEBOOK_METADATA_FILE = \"/opt/ml/metadata/resource-metadata.json\"\n",
    "domain_id = None\n",
    "\n",
    "if os.path.exists(NOTEBOOK_METADATA_FILE):\n",
    "    with open(NOTEBOOK_METADATA_FILE, \"rb\") as f:\n",
    "        md = json.loads(f.read())\n",
    "        domain_id = md.get('DomainId')\n",
    "        user_profile_name = md.get('UserProfileName')\n",
    "        \n",
    "        print(f\"SageMaker domain id: {domain_id}\\n\"\n",
    "              f\"User profile name: {user_profile_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17415753-c66d-4c17-93b3-1f3fb3ea9c78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'domain_id' (str)\n",
      "Stored 'user_profile_name' (str)\n",
      "Stored 'region' (str)\n",
      "Stored 'account_id' (str)\n",
      "Stored 'bucket_prefix' (str)\n",
      "Stored variables and their in-db values:\n",
      "account_id                             -> '949335012047'\n",
      "baseline_s3_url                        -> 's3://sagemaker-us-east-1-949335012047/from-idea-t\n",
      "bucket_name                            -> 'sagemaker-us-east-1-949335012047'\n",
      "bucket_prefix                          -> 'genai-on-aws-workshop'\n",
      "domain_id                              -> 'd-gg3vgzr0ftzx'\n",
      "evaluation_s3_url                      -> 's3://sagemaker-us-east-1-949335012047/from-idea-t\n",
      "experiment_name                        -> 'from-idea-to-prod-experiment-20-07-44-29'\n",
      "initialized                            -> True\n",
      "input_s3_url                           -> 's3://sagemaker-us-east-1-949335012047/from-idea-t\n",
      "model_package_group_name               -> 'from-idea-to-prod-model-group'\n",
      "output_s3_url                          -> 's3://sagemaker-us-east-1-949335012047/from-idea-t\n",
      "prediction_baseline_s3_url             -> 's3://sagemaker-us-east-1-949335012047/from-idea-t\n",
      "region                                 -> 'us-east-1'\n",
      "sm_role                                -> 'arn:aws:iam::949335012047:role/service-role/Amazo\n",
      "target_col                             -> 'y'\n",
      "test_s3_url                            -> 's3://sagemaker-us-east-1-949335012047/from-idea-t\n",
      "train_s3_url                           -> 's3://sagemaker-us-east-1-949335012047/from-idea-t\n",
      "user_profile_name                      -> 'canvas-demo-user'\n",
      "validation_s3_url                      -> 's3://sagemaker-us-east-1-949335012047/from-idea-t\n"
     ]
    }
   ],
   "source": [
    "%store domain_id\n",
    "%store user_profile_name\n",
    "%store region\n",
    "%store account_id\n",
    "%store bucket_prefix\n",
    "\n",
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca88f8f3-3de2-444a-a8ae-2ae0c6836b01",
   "metadata": {},
   "source": [
    "## Check quotas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99f6ab62-adf0-4d2c-bf63-550aa575b6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_quota(quota_code, min_v):\n",
    "    r = quotas_client.get_service_quota(\n",
    "        ServiceCode=\"sagemaker\",\n",
    "        QuotaCode=quota_code,\n",
    "    )\n",
    "    \n",
    "    q = r[\"Quota\"][\"Value\"]\n",
    "    n = r[\"Quota\"][\"QuotaName\"]\n",
    "\n",
    "    if q < min_v:\n",
    "        print (\n",
    "            f\"WARNING: Your quota {q} for {n} is less than required value of {min_v}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"SUCCESS: Your quota {q} for {n} is equal or more than required value of {min_v}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eb184b4-e2a0-4d82-b9cf-59a6f018a06a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Your quota 2.0 for ml.g5.2xlarge for endpoint usage is equal or more than required value of 1\n",
      "SUCCESS: Your quota 2.0 for ml.g5.12xlarge for endpoint usage is equal or more than required value of 1\n",
      "SUCCESS: Your quota 2.0 for ml.g5.48xlarge for endpoint usage is equal or more than required value of 0\n"
     ]
    }
   ],
   "source": [
    "quotas_client = boto3.client(\"service-quotas\")\n",
    "llm_instance_types = [\n",
    "    \"ml.g5.2xlarge\",  # needed for Falcon-7b deployment\n",
    "    \"ml.g5.12xlarge\", # needed for Falcon-40b deployment\n",
    "    \"ml.g5.48xlarge\", # needed for Falcon-40b deployment\n",
    "]\n",
    "                      \n",
    "quotas = {\n",
    "    \"ml.g5.2xlarge\": [\"L-9614C779\", 1],\n",
    "    \"ml.g5.12xlarge\": [\"L-65C4BD00\", 1],\n",
    "    \"ml.g5.48xlarge\": [\"L-0100B823\", 0],\n",
    "    \"ml.g4dn.xlarge\": [\"L-B67CFA0C\", 1],\n",
    "}\n",
    "     \n",
    "for i in llm_instance_types:\n",
    "    check_quota(quotas[i][0], quotas[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992436aa-8143-4e72-9c05-71033ed6222e",
   "metadata": {},
   "source": [
    "## Self-hosting LLM\n",
    "This section shows how to:\n",
    "1. Use SageMaker JumpStart to deploy a real-time endpoint with two lines of code\n",
    "2. Use SageMaker [real-time endpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html) to host an LLM\n",
    "3. Use SageMaker [asynchronous endpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html) to host an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f10d2212-c32f-4428-92dd-75faf22f9304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import HuggingFace classes\n",
    "from sagemaker.huggingface import (\n",
    "    get_huggingface_llm_image_uri, \n",
    "    HuggingFaceModel, \n",
    "    HuggingFacePredictor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11f8919-21b1-44a3-adc1-30d86e6c2a6e",
   "metadata": {},
   "source": [
    "### Deploy using SageMaker JumpStart\n",
    "The easiest option to deploy an LLM is to use [`JumpStartModel`](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.jumpstart.model.JumpStartModel) Python SDK class. Refer to [Introduction to SageMaker JumpStart - Text Generation with Falcon models](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-falcon.ipynb) for an example.\n",
    "\n",
    "For available models refer to the [JumpStart model list](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html).\n",
    "\n",
    "You deploy [Falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) LLM to a SageMaker real-time inference.\n",
    "\n",
    "If you have access to at least `ml.g5.12xlarge` instance for a real-time inference, you can deploy a bigger and more capable mode [Falcon-40B-instruct](https://huggingface.co/tiiuae/falcon-40b-instruct). \n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡   \n",
    "We recommend to use Falcon-40B-instruct for the retrieval augmented generation lab.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31ad4d2e-8ec6-40c8-a314-4273d376c597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b946eeca-c1c2-437e-b273-a2204e94d935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment the line for the model you'd like to use\n",
    "\n",
    "js_model_id = \"huggingface-llm-falcon-7b-bf16\"\n",
    "# js_model_id = \"huggingface-llm-falcon-40b-instruct-bf16\" # Falcon-40B-instruct\n",
    "\n",
    "js_model = JumpStartModel(model_id=js_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d5a150-c322-48f5-b12c-39b550b32af9",
   "metadata": {},
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi48L3RleHQ+Cjwvc3ZnPgo=\" alt=\"Time alert open medium\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de250164-6aa3-4df7-bb48-b5c7db9b7a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "predictor_js = js_model.deploy(tags=workshop_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1bcda4-8686-4cb3-82bc-76f7e8e1953c",
   "metadata": {},
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026c1655-f6cb-46a5-a1c3-8f643e3b131a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"\\nI'm going to Seattle for a week in July. I'm staying in the downtown area, but I'm not sure what to do. I'm not a big shopper, but I'm not opposed to it. I\"}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send request\n",
    "predictor_js.predict({\n",
    "\t\"inputs\": \"Hey Falcon! Any recommendations for my holidays in Seattle?\",\n",
    "    'parameters': {'max_new_tokens': 50,}\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa246933-0479-4745-b774-b3d93c408824",
   "metadata": {},
   "source": [
    "### Deploy using HuggingFace TGI\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ \n",
    "This section deploys another SageMaker real-time endpoint. If you've already deployed an endpoint using SageMaker JumpStart and don't want to deploy the second endpoint, skip this section.\n",
    "</div>\n",
    "\n",
    "If you'd like to use a model which is not onboarded to JumpStart, you can use a HuggingFace container.\n",
    "\n",
    "![](../static/img/hf-tgi.png)\n",
    "\n",
    "[HuggingFace LLM DLC](https://huggingface.co/blog/sagemaker-huggingface-llm) is a new purpose-built Inference Container to easily deploy LLMs in a secure and managed environment. The DLC is powered by Text Generation Inference (TGI), an open-source, purpose-built solution for deploying and serving Large Language Models (LLMs). TGI enables high-performance text generation using Tensor Parallelism and dynamic batching for the most popular open-source LLMs, including StarCoder, BLOOM, GPT-NeoX, Llama, and T5. Text Generation Inference is already used by customers such as IBM, Grammarly, and the Open-Assistant initiative implements optimization for all supported model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54550e3b-ded3-4306-868d-33b67821e38d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> ðŸ’¡   \n",
    "We recommend to use Falcon-40B-instruct for the retrieval augmented generation lab.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad0248b-1daa-41d5-8243-bfa96188c2bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment the settings for the model you'd like to use\n",
    "\n",
    "# Falcon-7b instruct\n",
    "# hf_model_id = 'tiiuae/falcon-7b-instruct'\n",
    "# llm_instance_type = 'ml.g5.2xlarge'\n",
    "# gpus = 1\n",
    "\n",
    "# Falcon-40b instruct\n",
    "hf_model_id = 'tiiuae/falcon-40b-instruct'\n",
    "llm_instance_type = 'ml.g5.12xlarge' # recommended ml.g5.48xlarge, gpus = 8\n",
    "gpus = 4 # or 8 if ml.g5.48xlarge\n",
    "\n",
    "tgi_image = get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\")\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "config = {\n",
    "\t'HF_MODEL_ID': hf_model_id,\n",
    "\t'SM_NUM_GPUS': json.dumps(gpus),\n",
    "    'HF_MODEL_QUANTIZE': \"bitsandbytes\", # Use quantization with ml.g5.12xlarge\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri=tgi_image,\n",
    "\tenv=config,\n",
    "\trole=sm_role, \n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {tgi_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99b5c63-1700-4127-8e1f-056408775d3f",
   "metadata": {},
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi48L3RleHQ+Cjwvc3ZnPgo=\" alt=\"Time alert open medium\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c19d721-13ce-437b-ac7f-d57726dc9238",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# deploy model to SageMaker real-time endpoint\n",
    "predictor_tgi = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type=llm_instance_type,\n",
    "\tcontainer_startup_health_check_timeout=1800,\n",
    "    tags=workshop_tags,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65df2ebb-5d15-436e-add0-1aaa3565daf3",
   "metadata": {},
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b7e7cc-ba8c-4b90-8cee-155d7d9b4f5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# send request\n",
    "predictor_tgi.predict({\n",
    "\t\"inputs\": \"Hey Falcon! Any recommendations for my holidays in Seattle?\",\n",
    "    'parameters': {'max_new_tokens': 500,}\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e8dd56-8424-46e8-9b59-9c4930ab970c",
   "metadata": {},
   "source": [
    "### Deploy ansynchrounous endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec751e78-a0ea-46c3-8f68-c5dd7c80fc7b",
   "metadata": {},
   "source": [
    "You can use SageMaker [asynchronous inference](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html) to host LLMs.\n",
    "\n",
    "SageMaker asynchronous inference queues incoming requests and processes them asynchronously. Asynchronous inference is suitable for workloads that do not have sub-second latency requirements and have relaxed latency requirements. You might require the asynchronous inference in two use cases:  \n",
    "1. If inference requests can take up to 15 minutes to process and if you have payload sizes of up to 1 GB  \n",
    "2. Asynchronous inference endpoints let you control costs by scaling down endpoints instance count to zero when they are idle, so you only pay when your endpoints are processing requests.\n",
    "\n",
    "Invocation of asynchronous endpoints differ from real-time endpoints. Rather than pass request payload inline with the request, you upload the payload to Amazon S3 and pass an Amazon S3 URI as a part of the request. Upon receiving the request, SageMaker provides you with a token with the output location where the result will be placed once processed. Internally, SageMaker maintains a queue with these requests and processes them. During endpoint creation, you can optionally specify an Amazon SNS topic to receive success or error notifications. Once you receive the notification that your inference request has been successfully processed, you can access the result in the output Amazon S3 location.\n",
    "\n",
    "The following diagram shows an overview of the end-to-end flow with Asynchronous inference endpoint.\n",
    "\n",
    "![](../static/img/sm-async-endpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9043656d-82d0-4e5d-8978-1d44708a3417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.async_inference.async_inference_config import AsyncInferenceConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8adf1b-8263-4795-a307-8acbab981199",
   "metadata": {},
   "source": [
    "Creation of an asynchronous endpoint isn't any different from real-time endpoints. You need to create a model, an endpoint configuration, and an endpoint. If you use [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/), the [Model](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html) class is convenient abstraction to create all needed structures with just one line of code by calling `deploy()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb0d80b-b9df-4d49-bb0f-b74276482e67",
   "metadata": {},
   "source": [
    "First, create a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9866a14d-9706-4603-9fd0-4574b2f8d434",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the settings for the model you'd like to use\n",
    "# Falcon-7b instruct\n",
    "hf_model_id = 'tiiuae/falcon-7b-instruct'\n",
    "llm_instance_type = 'ml.g5.2xlarge'\n",
    "gpus = 1\n",
    "\n",
    "# Falcon-40b instruct\n",
    "# hf_model_id = 'tiiuae/falcon-40b-instruct'\n",
    "# llm_instance_type = 'ml.g5.12xlarge' # recommended ml.g5.48xlarge, gpus = 8\n",
    "# gpus = 4 # or 8 if ml.g5.48xlarge\n",
    "\n",
    "tgi_image = get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\")\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "config = {\n",
    "\t'HF_MODEL_ID': hf_model_id,\n",
    "\t'SM_NUM_GPUS': json.dumps(gpus),\n",
    "    #Â 'HF_MODEL_QUANTIZE': \"bitsandbytes\", # Use quantization with ml.g5.12xlarge\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "async_inference_model = HuggingFaceModel(\n",
    "\timage_uri=tgi_image,\n",
    "\tenv=config,\n",
    "\trole=sm_role, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d24048-2446-4638-b745-ffab61fbff82",
   "metadata": {},
   "source": [
    "Second, configure [`AsyncInferenceConfig`](https://sagemaker.readthedocs.io/en/stable/api/inference/async_inference.html) to use in `model.depoy()`. In the inference config you can specify an S3 url where the endpoint uploads an inference output, an optional Amazon SNS notification config, and other settings like encryption key, or failure path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3c7b233-7c6f-43d2-a2ce-a82a02b6965f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_path = f\"s3://{bucket_name}/{bucket_prefix}/async-inference/output\"\n",
    "async_inference_config = AsyncInferenceConfig(\n",
    "    output_path=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b3c859-119e-490c-b1d2-5564968d43f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Third, create an asynchronous endpoint using `model.deploy()` method:\n",
    "\n",
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi48L3RleHQ+Cjwvc3ZnPgo=\" alt=\"Time alert open medium\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f890cbf-b67b-4df0-88d9-016935c21105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "# deploy model to SageMaker real-time endpoint\n",
    "predictor_async = async_inference_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type=llm_instance_type,\n",
    "    async_inference_config=async_inference_config, # important to use AsyncInferenceConfig to create an asyn endpoint\n",
    "\tcontainer_startup_health_check_timeout=1800,\n",
    "    tags=workshop_tags,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe5ade3-ba41-4427-855d-37fcaea486fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2175c28-84c8-42e3-8948-a5c826385d0d",
   "metadata": {},
   "source": [
    "You can use `predictor.predict()` method to send a request to an asynchronous endpoint. The Python SDK class implementation takes care about all required steps, such as uploading a request to Amazon S3, calling the SageMaker runtime API [`InvokeEndpointAsync`](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_runtime_InvokeEndpointAsync.html), waiting until the endpoint uploads an inference result to the output path, and returning the result back.\n",
    "\n",
    "If you use SageMaker API or boto3 SDK method [`invoke_endpoint_async`](https://boto3.amazonaws.com/v1/documentation/api/1.26.83/reference/services/sagemaker-runtime/client/invoke_endpoint_async.html), you need to implement all these steps. For an example refer to a sample [notebook](https://github.com/aws/amazon-sagemaker-examples/blob/main/async-inference/Async-Inference-Walkthrough.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dded56d-ea6f-4c96-9119-93954231317d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hey Falcon! Any recommendations for my holidays in Seattle?\\nThere are so many great things to do in Seattle! Some popular recommendations include visiting the Space Needle, hiking in the nearby mountains, and checking out the local food scene. You could also consider taking a tour of the city, or visiting some of the nearby islands for a change of scenery. Let me know if you have any specific interests or preferences, and I can help you come up with a plan!'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_async.predict({\n",
    "\t\"inputs\": \"Hey Falcon! Any recommendations for my holidays in Seattle?\",\n",
    "    'parameters': {'max_new_tokens': 500,}\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f26de2-4521-4194-9267-9d6fc09c8c6d",
   "metadata": {},
   "source": [
    "By running the next cell, you can check that the inference result is uploaded to the specified S3 url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4b738af-da79-4ab3-acaa-fbd3a7ae6b89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-30 20:28:48        485 genai-on-aws-workshop/async-inference/output/03f45e09-3dc7-4ae8-a4ca-a74964f50274.out\n",
      "2023-10-30 17:46:29        485 genai-on-aws-workshop/async-inference/output/10598ef0-a8a9-47da-9393-90439550c06b.out\n",
      "2023-10-31 10:49:28        485 genai-on-aws-workshop/async-inference/output/3387b96b-cdb3-42c9-9ef5-fefc5f1775bd.out\n",
      "2023-10-30 15:02:39        485 genai-on-aws-workshop/async-inference/output/662e57c1-3128-4c2b-8882-5a87575a6659.out\n",
      "2023-10-30 20:28:32        485 genai-on-aws-workshop/async-inference/output/cbe09475-d754-4488-a538-62b48434d231.out\n",
      "2023-10-31 09:01:11        485 genai-on-aws-workshop/async-inference/output/cee62680-c50c-46f7-b3c6-1c1d0144a416.out\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls {output_path} --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4fe82d-04d3-49c3-b1b8-bb9ce47f7f88",
   "metadata": {},
   "source": [
    "### Add auto scaling policy to the asynchronous endpoint\n",
    "Amazon SageMaker supports [automatic scaling (autoscaling) your asynchronous endpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-autoscale.html). Autoscaling dynamically adjusts the number of instances provisioned for a model in response to changes in your workload. Autoscaling works with both real-time and asynchronous endpoints, but with asynchonous inference you can scale down an endpoint to zero intances. When an endpoind receives requests when there are zero intances, they are queued for processing once the endpoint scales up. This section implements an autoscaling policy to scale to zero for the deployed asynchronous endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b28e98d-9361-4711-bdef-2bd91e9d4d06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    predictor_async.endpoint_name\n",
    "except NameError:\n",
    "    raise Exception(f\"ERROR: You haven't deployed an asynchronous endpoint!\")\n",
    "    \n",
    "ep = sm_client.describe_endpoint(EndpointName=predictor_async.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17a9654-df96-4720-912d-2551a79cb876",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, define a scaling policy. The scaling policy defines the desired scaling behavior in response to changes in metrics.\n",
    "For a list of possible scaling metrics refer to [Asynchronous Inference Endpoint Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-monitor.html#async-inference-monitor-cloudwatch-async). This configuration tracks the `ApproximateBacklogSizePerInstance` and `HasBacklogWithoutCapacity` metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82ed50b1-ca50-4007-8dd7-dc3a12f944ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TargetTrackingScalingPolicyConfiguration = {\n",
    "        'TargetValue': 2.0, # The target value for the metric. Here the metric is: ApproximateBacklogSizePerInstance\n",
    "        'CustomizedMetricSpecification': {\n",
    "            'MetricName': 'ApproximateBacklogSizePerInstance',\n",
    "            'Namespace': 'AWS/SageMaker',\n",
    "            'Dimensions': [\n",
    "                {'Name': 'EndpointName', 'Value': predictor_async.endpoint_name },\n",
    "            ],\n",
    "            'Statistic': 'Average',\n",
    "        },\n",
    "        'ScaleInCooldown': 300, # duration until scale in\n",
    "        'ScaleOutCooldown': 300 # duration between scale out\n",
    "    }            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a62b441f-6eb0-4f4a-aa0b-65e34bddbd85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "StepScalingPolicyConfiguration = {\n",
    "    \"AdjustmentType\": \"ChangeInCapacity\", # Specifies whether the ScalingAdjustment value in the StepAdjustment property is an absolute number or a percentage of the current capacity. \n",
    "    \"MetricAggregationType\": \"Average\", # The aggregation type for the CloudWatch metrics.\n",
    "    \"Cooldown\": 300, # The amount of time, in seconds, to wait for a previous scaling activity to take effect. \n",
    "    \"StepAdjustments\": # A set of adjustments that enable you to scale based on the size of the alarm breach.\n",
    "    [ \n",
    "        {\n",
    "          \"MetricIntervalLowerBound\": 0,\n",
    "          \"ScalingAdjustment\": 1\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cf0531-fe79-4bff-a54c-71a66664eba3",
   "metadata": {},
   "source": [
    "Second, register the endpoint as a target for autoscaling. The `MinCapacity` is zero, which means the endpoint can be scaled down to zero instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b827345-e1d6-4a88-9754-a807764264f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aa = boto3.client(\"application-autoscaling\")\n",
    "cw = boto3.client(\"cloudwatch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "281ff5a3-aa81-4e79-a445-7813a9a06e80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# you need to use this format for resource_id\n",
    "resource_id = (\"endpoint/\" + predictor_async.endpoint_name + \"/variant/\" + ep['ProductionVariants'][0]['VariantName'])\n",
    "\n",
    "# Configure Autoscaling on asynchronous endpoint down to zero instances\n",
    "r = aa.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MinCapacity=0,\n",
    "    MaxCapacity=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3507b8a7-1f1e-4c10-a621-38ad49a3b6fd",
   "metadata": {},
   "source": [
    "Third, apply scaling policies to the endpoint. We apply two policies, one for tracking the number of requests in queue and scale the endpoint based on it, and the second to scale the endpoint up from zero instances if there is at least one request waiting in the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "392e8d76-e2c8-4e31-9a1e-991efdcb50e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r = aa.put_scaling_policy(\n",
    "    PolicyName=f\"ApproximateBacklogSizePerInstance-ScalingPolicy-{predictor_async.endpoint_name}\",\n",
    "    ServiceNamespace=\"sagemaker\",  # The namespace of the service that provides the resource.\n",
    "    ResourceId=resource_id,  # Endpoint production variant\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",  # SageMaker supports only Instance Count\n",
    "    PolicyType=\"TargetTrackingScaling\",  # 'StepScaling' or 'TargetTrackingScaling'\n",
    "    TargetTrackingScalingPolicyConfiguration=TargetTrackingScalingPolicyConfiguration,    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d59a6fcc-e962-4d55-b099-5fce8e152949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r = aa.put_scaling_policy(\n",
    "    PolicyName=f\"HasBacklogWithoutCapacity-ScalingPolicy-{predictor_async.endpoint_name}\",\n",
    "    ServiceNamespace=\"sagemaker\",  # The namespace of the service that provides the resource.\n",
    "    ResourceId=resource_id,  # Endpoint production variant\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",  # SageMaker supports only Instance Count\n",
    "    PolicyType=\"StepScaling\",  # 'StepScaling' or 'TargetTrackingScaling'\n",
    "    StepScalingPolicyConfiguration=StepScalingPolicyConfiguration,    \n",
    ")\n",
    "\n",
    "step_scaling_policy_arn = r['PolicyARN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6ad28-f2ff-4fce-a43a-9ee4f658281d",
   "metadata": {},
   "source": [
    "Finally, create a CloudWatch alarm with the SageMaker metric `HasBacklogWithoutCapacity`. When this allarm is triggered, it initiates the step scaling policy we defined in the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ede7551-df7c-475e-9b89-83c2ccb360e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r = cw.put_metric_alarm(\n",
    "    AlarmName=f\"StepTracking-endpoint/{predictor_async.endpoint_name}/variant/{ep['ProductionVariants'][0]['VariantName']}-AlarmStepUp-{uuid.uuid4()}\",\n",
    "    MetricName='HasBacklogWithoutCapacity',\n",
    "    Namespace='AWS/SageMaker',\n",
    "    Statistic='Average',\n",
    "    EvaluationPeriods= 2,\n",
    "    DatapointsToAlarm= 2,\n",
    "    Threshold= 1,\n",
    "    ComparisonOperator='GreaterThanOrEqualToThreshold',\n",
    "    TreatMissingData='missing',\n",
    "    Dimensions=[\n",
    "        { 'Name':'EndpointName', 'Value': predictor_async.endpoint_name },\n",
    "    ],\n",
    "    Period= 60,\n",
    "    AlarmActions=[step_scaling_policy_arn]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36751888-5834-4fda-ae18-50ee15a5debc",
   "metadata": {},
   "source": [
    "Now the endpoint will be scaled down to zero instances if there is no requests in the queue and scaled up from zero if there is at least one request waiting in the queue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8002d73-b103-4643-b009-99d0ad95cc04",
   "metadata": {},
   "source": [
    "### Use asynchronous endpoints in LangChain\n",
    "SageMaker asynchronous endpoints are a cost-optimized solution when you need to host an LLM but traffic is inpredictable. While you can use SageMaker Python SDK classes [`Predictor`](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html) and [`AsyncPredictor`](https://sagemaker.readthedocs.io/en/stable/api/inference/predictor_async.html) to access asynchronous endpoints, LangChain class [`SageMakerEndpoint`](https://python.langchain.com/docs/integrations/llms/sagemaker) doesn't support asynchronous inference.\n",
    "\n",
    "This workshop contains an implementation of `SageMakerEndpointAsync` class you can use in LangChain framework. The code is in the `llm/sagemaker_async_endpoint.py` file. The implementation is based on [this GitHub repository](https://github.com/dgallitelli/langchain/blob/master/langchain/llms/sagemaker_async_endpoint.py).\n",
    "\n",
    "Run the following cells to test LangChain with `SageMakerEndpointAsync` abstraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e92d601f-4c00-4e2b-a6f2-a9faafbf4821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains import LLMChain\n",
    "from llm.sagemaker_async_endpoint import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1dafe199-cf76-4b2d-904f-ab526c4452e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_bucket = bucket_name\n",
    "input_prefix = f\"{bucket_prefix}/async-endpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80071730-3ce9-418d-916b-0ee39459b419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type:str = \"application/json\"\n",
    "    accepts:str = \"application/json\"\n",
    "    len_prompt:int = 0\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        self.len_prompt = len(prompt)\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": {\"max_new_tokens\": 100, \"do_sample\": False, \"repetition_penalty\": 1.1}})\n",
    "        return input_str.encode('utf-8')\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = output.read()\n",
    "        res = json.loads(response_json)\n",
    "        ans = res[0]['generated_text']\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5c957a0-1dc1-4e87-89b2-8559acfe26f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(\n",
    "    llm=SagemakerAsyncEndpoint(\n",
    "        input_bucket=input_bucket,\n",
    "        input_prefix=input_prefix,\n",
    "        endpoint_name=predictor_async.endpoint_name,\n",
    "        region_name=sagemaker.Session().boto_region_name,\n",
    "        content_handler=ContentHandler(),\n",
    "    ),\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"query\"],\n",
    "        template=\"{query}\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac7b2c6-5cde-4ab0-a0fe-63a7866a0440",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> ðŸ’¡ \n",
    "If the async endpoint scaled to zero, the following cell raises an exception \"The endpoint is not running\". The endpoint wakes up automatically and is available after approx 10 min. You just re-run the cell after the endoint is in InService status again.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "34f92f8a-7110-4207-a4fd-bcc412d92c18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey Falcon! Any recommendations for my holidays in Seattle?\\nThere are so many great things to do in Seattle! Some popular recommendations include visiting the Space Needle, hiking in the nearby mountains, and checking out the local food scene. You could also consider taking a tour of the city, or exploring some of the nearby neighborhoods like Ballard or Fremont. Let me know if you have any specific interests or preferences!'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Hey Falcon! Any recommendations for my holidays in Seattle?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c705e0-b0cd-4b2c-897c-45b09c1a25ba",
   "metadata": {},
   "source": [
    "## Use Amazon Bedrock to access LLM\n",
    "The easiest way to access different LLMs is to use Amazon Bedrock managed API. With Amazon Bedrock you don't need to create and maintain an inference endpoint. You can access Amazon and third-party models.\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "You must request access to a model before you can use it. If you try to use the model (with the API or console) before you have requested access to it, you receive an error message. For more information, seeÂ <a href=https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html>Model access</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30830d43-ca40-4c60-a407-66e47e4265f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make sure you have a boto3 version that supports Amazon Bedrock\n",
    "assert(boto3.__version__ >= '1.28.57')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436617a9-38c8-49de-9e81-0cff2b4e22e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock = boto3.client(service_name='bedrock')\n",
    "\n",
    "#Â list all available models you have access to\n",
    "bedrock.list_foundation_models()[\"modelSummaries\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2995e0d2-b500-4fd8-a42d-4a9790bf147c",
   "metadata": {},
   "source": [
    "To use the model you need to provide an Amazon Bedrock model id. The list of available model ids you can find in Amazon Bedrock [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c3df98-6e55-481c-be26-3809fe85e7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_model_id = 'anthropic.claude-instant-v1'\n",
    "llm = bedrock.get_foundation_model(modelIdentifier=bedrock_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d754693-dd57-4078-9bdc-34248c66cef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime')\n",
    "body = json.dumps({\n",
    "    \"prompt\": \"\\n\\nHuman:explain black holes to 8th graders\\n\\nAssistant:\",\n",
    "    \"max_tokens_to_sample\": 300,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.9,\n",
    "})\n",
    "\n",
    "modelId = llm['modelDetails']['modelId']\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, \n",
    "    modelId=modelId, \n",
    "    accept=accept, \n",
    "    contentType=contentType\n",
    ")\n",
    "\n",
    "response_body = json.loads(response.get('body').read())\n",
    "\n",
    "# text\n",
    "print(response_body.get('completion'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed34af2-bd71-4b11-9e47-fc2a6b60f6fd",
   "metadata": {},
   "source": [
    "##Â Test the LLM endponts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9068ed4-c6b2-40db-9413-e3cd590f481b",
   "metadata": {},
   "source": [
    "If you successfully deployed one or more SageMaker LLM endpoints, you can chat with the deployed LLMs. For creating UX around the LLM you can use [Gradio](https://gradio.app/) Python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327862ab-8b3e-4ff8-a538-efdb1751023d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio_app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90163154-68b3-493b-84de-506616d94890",
   "metadata": {
    "tags": []
   },
   "source": [
    "Configure and create Gradio chatbot application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0861d99-ce5d-452f-b6c9-dfc2735d71a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters for llm\n",
    "parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.8,\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"stop\": [\"\\nUser:\", \"<|endoftext|>\", \" User:\", \"###\"],\n",
    "}\n",
    "\n",
    "system_prompt = \"You are an helpful Assistant, called LLM. Knowing everyting about AWS.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7590a57c-0c27-438e-8b51-cdcff280ccca",
   "metadata": {},
   "source": [
    "List all endpoints you created in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0876f3f8-44ec-46e8-b213-ede4de256297",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_workshop_endpoints():\n",
    "    workshop_endpoints = []\n",
    "    workshop_tag_key = workshop_tags[0]['Key']\n",
    "    workshop_tag_value = workshop_tags[0]['Value']\n",
    "\n",
    "    for ep in sm_client.list_endpoints().get('Endpoints'):\n",
    "        ep_name = ep['EndpointName']\n",
    "        en_arn = ep['EndpointArn']\n",
    "        v = [d['Value'] for d in sm_client.list_tags(ResourceArn=en_arn)['Tags'] if d['Key'] == workshop_tag_key]\n",
    "        if len(v) and v[0] == workshop_tag_value:\n",
    "            workshop_endpoints.append(ep_name)\n",
    "\n",
    "    return workshop_endpoints\n",
    "\n",
    "def describe_workshop_endpoints():\n",
    "    for i, ep_name in enumerate(list_workshop_endpoints()):\n",
    "        ep_config_name = sm_client.describe_endpoint(EndpointName=ep_name)['EndpointConfigName']\n",
    "        ep_config = sm_client.describe_endpoint_config(EndpointConfigName=ep_config_name)\n",
    "\n",
    "        instance = ep_config['ProductionVariants'][0]['InstanceType']\n",
    "        m_name = ep_config['ProductionVariants'][0]['ModelName']\n",
    "        is_async = ep_config.get('AsyncInferenceConfig') != None\n",
    "\n",
    "        print(f\"endpoint {i} ({'async' if is_async else 'real-time'}): {ep_name} ({instance}) -> model: {m_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91a18e26-31cc-4962-b918-f1119155c483",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint 0 (async): huggingface-pytorch-tgi-inference-2023-10-31-08-46-21-240 (ml.g5.2xlarge) -> model: huggingface-pytorch-tgi-inference-2023-10-31-08-46-20-523\n",
      "endpoint 1 (real-time): hf-llm-falcon-7b-bf16-2023-10-31-08-37-14-377 (ml.g5.2xlarge) -> model: hf-llm-falcon-7b-bf16-2023-10-31-08-37-13-274\n"
     ]
    }
   ],
   "source": [
    "describe_workshop_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a30f8-7a87-4951-9fd4-974739247b48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use any of existing endpoints from the list above\n",
    "if len(list_workshop_endpoints()) > 0:\n",
    "    endpoint_name_to_use = list_workshop_endpoints()[0]\n",
    "else:\n",
    "    raise Exception(f\"You don't have any SageMaker endpoints. You need to create one to run Gradio chatbot\")\n",
    "\n",
    "boto_session=boto3.session.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34619afa-0ebb-44e5-95f1-e7d68e3ad7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You cannot use asynchronous endpoint with Gradio, choose a real-time endpoint instead\n",
    "if sm_client.describe_endpoint(EndpointName=endpoint_name_to_use).get('AsyncInferenceConfig'):\n",
    "    raise Exception(f\"You cannot use an async endpoint with Gradio, choose a real-time endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5329f15-8bdf-4e22-aeee-6e3b371e63ca",
   "metadata": {},
   "source": [
    "The cell below create a chat window where you can play with the model by asking questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b9672d-d9be-4c0b-944b-79e74dc54099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create gradio app\n",
    "gradio_app.create_gradio_app(\n",
    "    endpoint_name_to_use,\n",
    "    session=boto_session,\n",
    "    parameters=parameters, \n",
    "    system_prompt=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a2876-56ea-4c0d-8977-dc864cc6ceee",
   "metadata": {},
   "source": [
    "## Use endpoints in workhop labs\n",
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "If you need an endpoint name to use in a workshop lab, the following cell prints all deployed endpoints:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97e1e4db-f725-4d3c-8939-92d287185913",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint 0 (async): huggingface-pytorch-tgi-inference-2023-10-31-08-46-21-240 (ml.g5.2xlarge) -> model: huggingface-pytorch-tgi-inference-2023-10-31-08-46-20-523\n"
     ]
    }
   ],
   "source": [
    "describe_workshop_endpoints()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05b697a-5245-4ad5-8823-81b284eaadf7",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "You must remove deployed endpoints after you completed workshop labs might use them to avoid unexpected costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44582c1f-c6de-47a1-b320-baf7194fa409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "def delete_by_endpoint_name(ep_name):\n",
    "    print(f\"The endpoint {ep_name} will be deleted!\")\n",
    "    print(\"Are you sure you want to delete this endpoint? (y/n)\")\n",
    "    \n",
    "    if input() == 'y':\n",
    "        print(f\"Deleting {ep_name}\")\n",
    "        predictor = Predictor(endpoint_name=ep_name)\n",
    "        predictor.delete_model()\n",
    "        predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dee8f73a-83af-4d6f-8804-d416bef4bdbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The endpoint huggingface-pytorch-tgi-inference-2023-11-10-19-08-26-786 will be deleted!\n",
      "Are you sure you want to delete this endpoint? (y/n)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting huggingface-pytorch-tgi-inference-2023-11-10-19-08-26-786\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "The endpoint huggingface-pytorch-tgi-inference-2023-10-31-08-46-21-240 will be deleted!\n",
      "Are you sure you want to delete this endpoint? (y/n)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " n\n"
     ]
    }
   ],
   "source": [
    "for ep in list_workshop_endpoints():\n",
    "    delete_by_endpoint_name(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6e1b0a3-55da-4d60-925b-6aa2c930331d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint 0 (async): huggingface-pytorch-tgi-inference-2023-10-31-08-46-21-240 (ml.g5.2xlarge) -> model: huggingface-pytorch-tgi-inference-2023-10-31-08-46-20-523\n"
     ]
    }
   ],
   "source": [
    "describe_workshop_endpoints()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f93bf5e-3878-47be-a9f7-e3c53e41dc9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Shutdown Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440715f7-ab02-4e1a-90d4-5e16b690114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.0-cpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
